%YAML 1.2
# Copyright (c) 2022, NVIDIA CORPORATION.  All rights reserved.
#
# NVIDIA CORPORATION and its licensors retain all intellectual property
# and proprietary rights in and to this software, related documentation
# and any modifications thereto.  Any use, reproduction, disclosure or
# distribution of this software and related documentation without an express
# license agreement from NVIDIA CORPORATION is strictly prohibited.
---
name: triton_server
components:
- name: server
  type: nvidia::triton::TritonServer
  parameters:
    log_level: 2
    model_repository_paths: [model_repository_path_placeholder]
    backend_directory_path: /opt/tritonserver/backends
---
name: triton_request
components:
- name: input
  type: nvidia::gxf::DoubleBufferReceiver
  parameters:
    capacity: 12
    policy: 0
- type: nvidia::gxf::MessageAvailableSchedulingTerm
  parameters:
    receiver: input
    min_size: 1
- name: inferencer_impl
  type: nvidia::triton::TritonInferencerImpl
  parameters:
    server: triton_server/server
    model_name: model_name_placeholder
    model_version: 1
    num_concurrent_requests: 10
    max_batch_size: 0
    async_scheduling_term: triton_response/async_st
    inference_mode: Direct
- name: requester
  type: nvidia::triton::TritonInferenceRequest
  parameters:
    inferencer: inferencer_impl
    dummy_rx: input
    rx: [input]
    input_tensor_names: [input_tensor_name_placeholder]
    input_binding_names: [input_binding_name_placeholder]
- name: triton_request_receptive_scheduling_term
  type: nvidia::triton::TritonRequestReceptiveSchedulingTerm
  parameters:
    inferencer: inferencer_impl
---
name: triton_response
components:
- name: output
  type: nvidia::gxf::DoubleBufferTransmitter
  parameters:
    capacity: 12
    policy: 0
- type: nvidia::gxf::DownstreamReceptiveSchedulingTerm
  parameters:
    transmitter: output
    min_size: 1
- type: nvidia::triton::TritonInferenceResponse
  parameters:
    inferencer: triton_request/inferencer_impl
    tx: output
    output_tensor_names: [output_tensor_name_placeholder]
    output_binding_names: [output_binding_name_placeholder]
- name: async_st
  type: nvidia::gxf::AsynchronousSchedulingTerm
---
name: tensor_copier
components:
- name: input
  type: nvidia::gxf::DoubleBufferReceiver
  parameters:
    capacity: 12
    policy: 0
- type: nvidia::gxf::MessageAvailableSchedulingTerm
  parameters:
    receiver: input
    min_size: 1
- name: output
  type: nvidia::gxf::DoubleBufferTransmitter
  parameters:
    capacity: 12
    policy: 0
- type: nvidia::gxf::DownstreamReceptiveSchedulingTerm
  parameters:
    transmitter: output
    min_size: 1
- name: allocator
  type: nvidia::gxf::UnboundedAllocator
- type: nvidia::gxf::TensorCopier
  parameters:
    receiver: input
    transmitter: output
    allocator: allocator
    mode: 0
---
name: vault
components:
- name: signal
  type: nvidia::gxf::DoubleBufferReceiver
  parameters:
    capacity: 1
    policy: 0
- type: nvidia::gxf::MessageAvailableSchedulingTerm
  parameters:
    receiver: signal
    min_size: 1
- name: vault
  type: nvidia::gxf::Vault
  parameters:
    source: signal
    max_waiting_count: 1
    drop_waiting: false
---
components:
- type: nvidia::gxf::Connection
  parameters:
    source: triton_response/output
    target: tensor_copier/input
- type: nvidia::gxf::Connection
  parameters:
    source: tensor_copier/output
    target: vault/signal
---
components:
- type: nvidia::gxf::GreedyScheduler
  parameters:
    clock: clock
    stop_on_deadlock: false
- name: clock
  type: nvidia::gxf::RealtimeClock
